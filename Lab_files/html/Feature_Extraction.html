
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>DT2119 - Speech and speaker recognition - Lab 1 - Feature extraction</title><meta name="generator" content="MATLAB 9.1"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2017-04-02"><meta name="DC.source" content="Feature_Extraction.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>DT2119 - Speech and speaker recognition - Lab 1 - Feature extraction</h1><!--introduction--><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#3">1.1 Enframe the audio signal</a></li><li><a href="#7">1.2 Pre-emphasys</a></li></ul></div><p>Mel Frequency Cepstral Coefficients (MFCCs) are coefficients used in Speech recognition based on human auditive perception. These coefficients come from the need, in the field of aoutonomous audio recognition, to extract the main features of an audio signal while discarding all the irrelevant features that will make the recognition harder to achieve (background noise, emotion ...)</p><p>The sounds generated by a human depend on the shape of the vocal tract, position of the tongue, teeth, lips etc. Then, if we can determine this shape accurately, the phoneme that was produced will be easy to identify. The main objetive of the MFCC it to accurately represent the envelope of the short time power spectrum in order to stablish the shape of the vocal tract. Consisting then of one of the most important features in Speech recognition</p><p>First we need to prepare the workspace and load the data set</p><pre class="codeinput">clear <span class="string">all</span>;
close <span class="string">all</span>;
load(<span class="string">'tidigits.mat'</span>)
load(<span class="string">'example.mat'</span>)
</pre><h2 id="3">1.1 Enframe the audio signal</h2><p>The purpose of this first step is to cut the original sample in many smaller ones. In this case, we want windowframes of 20 ms with a shift between windows of 10 ms. As the shift is smaller than the window, the computed frames will have shared parts between each other</p><p><i><b>Compute the number of samples per frame and shift</b></i></p><p>The sampling rate is S = 20 kHz. Then the period is <img src="Feature_Extraction_eq08067570237043316922.png" alt="$T = 1/S$">. We have that:</p><p>Length of the window: <img src="Feature_Extraction_eq09482477169178125687.png" alt="$0.02T = 400$"> samples Length of the shift: <img src="Feature_Extraction_eq11115082160516153914.png" alt="$0.01T = 200$"> samples</p><pre class="codeinput">winlen = 400;
winshift = 200;
</pre><p>Now is time to check the performance of the written function. To do so, the function is evaluated with the example's structure data set, and compared to the frames given in the example's structure frames. The output generated must be identical to the frames stored in the data structure in example.</p><pre class="codeinput">samples = example{1,1}.samples;
test = example{1,1}.frames;
frames = enframe(samples,winlen,winshift);
<span class="comment">% As the image have to be plotted in with the same orientation as in the</span>
<span class="comment">% test data then</span>
frames = frames(:,1:400)';
</pre><p>The comparison is shown in the folowwing figure.</p><pre class="codeinput">subplot(2,1,1)
imagesc(frames)
colormap <span class="string">jet</span>
title(<span class="string">' Enframe function output'</span>)
subplot(2,1,2)
imagesc(test')
colormap <span class="string">jet</span>
title(<span class="string">' Test frames set'</span>)
</pre><img vspace="5" hspace="5" src="Feature_Extraction_01.png" alt=""> <p>To check the performance numerically, we will substract the test data from the output and get the total differences between both of them</p><pre class="codeinput">subs = test' - frames;
Enframe_error = sum(sum(subs));
</pre><h2 id="7">1.2 Pre-emphasys</h2><p>The main objetive of this function is to compesate the 6dB/octave that are dropped due to the radiation at the lips. This funcion is:</p><p><img src="Feature_Extraction_eq01683044689347843080.png" alt="$y[n] = x[n] - ax[n-1]$"></p><p>Being the coefficientes  <img src="Feature_Extraction_eq12755552376203169728.png" alt="$A = 1$"> and <img src="Feature_Extraction_eq18042384864992526626.png" alt="$B = [1 -a]$">.</p><p>The purpose of substracting the main part of the previous input is because the important features are in the higher frequencias, remaining the lower frequencies almost unchenged. Therefor, doing the substraction we discard the similar parts of the signal analyzing only the higher frequencies, where the important features are.</p><p>We have now to check the performance of the written function. To do so, we have to compare the output of this function with the test data.</p><pre class="codeinput">a = 0.97;
preemph = preemp(frames,a);
test_preem = example{1,1}.preemph;
</pre><p>The comparison is shown in the folowwing figure.</p><pre class="codeinput">figure;
subplot(2,1,1)
imagesc(preemph)
colormap <span class="string">jet</span>
title(<span class="string">' Preemp function output'</span>)
subplot(2,1,2)
imagesc(test_preem')
colormap <span class="string">jet</span>
title(<span class="string">' Test preemphashis set'</span>)
</pre><img vspace="5" hspace="5" src="Feature_Extraction_02.png" alt=""> <p>To check the performance numerically, we will substract the test data from the output and get the total differences between both of them</p><pre class="codeinput">subs = test_preem' - preemph;
Preemphasis_error = sum(sum(subs))
</pre><pre class="codeoutput">
Preemphasis_error =

     0

</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2016b</a><br></p></div><!--
##### SOURCE BEGIN #####
%% DT2119 - Speech and speaker recognition - Lab 1 - Feature extraction

%%
% Mel Frequency Cepstral Coefficients (MFCCs) are coefficients used in
% Speech recognition based on human auditive perception. These coefficients
% come from the need, in the field of aoutonomous audio recognition, to
% extract the main features of an audio signal while discarding all the
% irrelevant features that will make the recognition harder to achieve
% (background noise, emotion ...)
%
% The sounds generated by a human depend on the shape of the vocal tract,
% position of the tongue, teeth, lips etc. Then, if we can determine this
% shape accurately, the phoneme that was produced will be easy to identify.
% The main objetive of the MFCC it to accurately represent the envelope of
% the short time power spectrum in order to stablish the shape of the vocal
% tract. Consisting then of one of the most important features in Speech
% recognition

%%
% First we need to prepare the workspace and load the data set
clear all;
close all; 
load('tidigits.mat')
load('example.mat')


%% 1.1 Enframe the audio signal
%
% The purpose of this first step is to cut the original sample in many
% smaller ones. In this case, we want windowframes of 20 ms with a shift
% between windows of 10 ms. As the shift is smaller than the window, the
% computed frames will have shared parts between each other
%
% _*Compute the number of samples per frame and shift*_
% 
% The sampling rate is S = 20 kHz. Then the period is $T = 1/S$. 
% We have that: 
%
% Length of the window: $0.02T = 400$ samples
% Length of the shift: $0.01T = 200$ samples


winlen = 400;
winshift = 200; 

%%
% Now is time to check the performance of the written function. To do so,
% the function is evaluated with the example's structure data set, and compared to the
% frames given in the example's structure frames. The output generated must
% be identical to the frames stored in the data structure in example.

samples = example{1,1}.samples; 
test = example{1,1}.frames;
frames = enframe(samples,winlen,winshift);
% As the image have to be plotted in with the same orientation as in the
% test data then
frames = frames(:,1:400)'; 
%% 
% The comparison is shown in the folowwing figure. 
subplot(2,1,1)
imagesc(frames)
colormap jet
title(' Enframe function output')
subplot(2,1,2)
imagesc(test')
colormap jet
title(' Test frames set')

%%
% To check the performance numerically, we will substract the test data
% from the output and get the total differences between both of them

subs = test' - frames;
Enframe_error = sum(sum(subs));
%% 1.2 Pre-emphasys
%
% The main objetive of this function is to compesate the 6dB/octave that
% are dropped due to the radiation at the lips. This funcion is: 
%
% $y[n] = x[n] - ax[n-1]$
%
% Being the coefficientes  $A = 1$ and $B = [1 -a]$.
%
% The purpose of substracting the main part of the previous input is
% because the important features are in the higher frequencias, remaining
% the lower frequencies almost unchenged. Therefor, doing the substraction
% we discard the similar parts of the signal analyzing only the higher
% frequencies, where the important features are. 
%
% We have now to check the performance of the written function. To do so,
% we have to compare the output of this function with the test data. 

a = 0.97;
preemph = preemp(frames,a);
test_preem = example{1,1}.preemph;

%% 
% The comparison is shown in the folowwing figure. 
figure;
subplot(2,1,1)
imagesc(preemph)
colormap jet
title(' Preemp function output')
subplot(2,1,2)
imagesc(test_preem')
colormap jet
title(' Test preemphashis set')

%%
% To check the performance numerically, we will substract the test data
% from the output and get the total differences between both of them

subs = test_preem' - preemph;
Preemphasis_error = sum(sum(subs))
##### SOURCE END #####
--></body></html>