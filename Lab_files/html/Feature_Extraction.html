
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>DT2119 - Speech and speaker recognition - Lab 1 - Feature extraction</title><meta name="generator" content="MATLAB 9.1"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2017-04-07"><meta name="DC.source" content="Feature_Extraction.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>DT2119 - Speech and speaker recognition - Lab 1 - Feature extraction</h1><!--introduction--><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#3">1.1 Enframe the audio signal</a></li><li><a href="#7">1.2 Pre-emphasys</a></li><li><a href="#10">1.3 Hamming window</a></li><li><a href="#15">1.4 Fast Fourier Transform</a></li><li><a href="#19">1.5 Mel filterbank log spectrum</a></li><li><a href="#23">1.6 Cosine Transform</a></li><li><a href="#26">1.7 Liftering</a></li><li><a href="#30">1.8 Calculation of the MFCCs for each uterance</a></li><li><a href="#31">2. Study the correlation between uterances</a></li><li><a href="#34">3. Distances</a></li><li><a href="#39">Linkage clustering</a></li><li><a href="#41">Train</a></li></ul></div><p>Mel Frequency Cepstral Coefficients (MFCCs) are coefficients used in Speech recognition based on human auditive perception. These coefficients come from the need, in the field of autonomous audio recognition, to extract the main features of an audio signal while discarding all the irrelevant features that will make the recognition harder to achieve (background noise, emotion ...)</p><p>The sounds generated by a human depend on the shape of the vocal tract, position of the tongue, teeth, lips etc. Then, if we can determine this shape accurately, the phoneme that was produced will be easy to identify. The main objetive of the MFCC it to accurately represent the envelope of the short time power spectrum in order to stablish the shape of the vocal tract. Consisting then of one of the most important features in Speech recognition</p><p>First we need to prepare the workspace and load the data set</p><pre class="codeinput">clear <span class="string">all</span>;
close <span class="string">all</span>;
load(<span class="string">'tidigits.mat'</span>)
load(<span class="string">'example.mat'</span>)
</pre><h2 id="3">1.1 Enframe the audio signal</h2><p>The purpose of this first step is to cut the original sample in many smaller ones. In this case, we want windowframes of 20 ms with a shift between windows of 10 ms. As the shift is smaller than the window, the computed frames will have shared parts between each other</p><p><b>Compute the number of samples per frame and shift</b></p><p>As out signal has been sampled, we dont have a continuos signal but a discrete one. This is why we are interested of knowing the number of samples per window rather than the time.</p><p>The sampling rate is S = 20 kHz. Then the period is <img src="Feature_Extraction_eq08067570237043316922.png" alt="$T = 1/S$">. We have that:</p><p>Length of the window: <img src="Feature_Extraction_eq09482477169178125687.png" alt="$0.02T = 400$"> samples Length of the shift: <img src="Feature_Extraction_eq11115082160516153914.png" alt="$0.01T = 200$"> samples</p><pre class="codeinput">winlen = 400;
winshift = 200;
</pre><p>Now is time to check the performance of the written function. To do so, the function is evaluated with the example's structure data set, and compared to the frames given in the example's structure frames. The output generated must be identical to the frames stored in the data structure in example.</p><pre class="codeinput">samples = example{1,1}.samples;
test = example{1,1}.frames';
frames = enframe(samples,winlen,winshift);
<span class="comment">% As the image have to be plotted in with the same orientation as in the</span>
<span class="comment">% test data then</span>
frames = frames(:,1:400)';
</pre><p>The comparison is shown in the folowwing figure.</p><pre class="codeinput">subplot(2,1,1)
imagesc(frames)
colormap <span class="string">jet</span>
title(<span class="string">' Enframe function output'</span>)
subplot(2,1,2)
imagesc(test)
colormap <span class="string">jet</span>
title(<span class="string">' Test frames set'</span>)
</pre><img vspace="5" hspace="5" src="Feature_Extraction_01.png" alt=""> <p>To check the performance numerically, we will substract the test data from the output and get the total differences between both of them</p><pre class="codeinput">subs = test - frames;
Enframe_error = sum(sum(subs))
</pre><pre class="codeoutput">
Enframe_error =

     0

</pre><h2 id="7">1.2 Pre-emphasys</h2><p>The main objetive of this function is to compesate the 6dB/octave that are dropped due to the radiation at the lips. This funcion is:</p><p><img src="Feature_Extraction_eq01683044689347843080.png" alt="$y[n] = x[n] - ax[n-1]$"></p><p>Being the coefficientes  <img src="Feature_Extraction_eq12755552376203169728.png" alt="$A = 1$"> and <img src="Feature_Extraction_eq18042384864992526626.png" alt="$B = [1 -a]$">.</p><p>The purpose of this filter is to filter the lower frequencies, letting pass the higher ones. This is because the important features are in the higher frequencies, remaining the lower frequencies almost unchanged. Therefore, by doing the filtering we discard the similar parts of the signal analyzing only the higher frequencies, where the important features are.</p><p>We have now to check the performance of the written function. To do so, we have to compare the output of this function with the test data.</p><pre class="codeinput">a = 0.97;
preemph = preemp(frames,a);
test_preem = example{1,1}.preemph';
</pre><p>The comparison is shown in the following figure.</p><pre class="codeinput">figure;
subplot(2,1,1)
imagesc(preemph)
colormap <span class="string">jet</span>
title(<span class="string">' Preemp function output'</span>)
subplot(2,1,2)
imagesc(test_preem)
colormap <span class="string">jet</span>
title(<span class="string">' Test preemphashis set'</span>)
</pre><img vspace="5" hspace="5" src="Feature_Extraction_02.png" alt=""> <p>To check the performance numerically, we will substract the test data from the output and get the total differences between both of them</p><pre class="codeinput">subs = test_preem - preemph;
Preemphasis_error = sum(sum(subs))
</pre><pre class="codeoutput">
Preemphasis_error =

     0

</pre><h2 id="10">1.3 Hamming window</h2><p>The use of the Hamming window is justified as it emphasizes the center of the window, improving this way the values of the Fourier Transfer Function. The sidelobes are reduced, being the center lobe much more important than these side lobes.</p><p>The Hamming window is used in order to reduce the discontinuities in the edges of the frames, focusing in the main(centered) frequencies.</p><p>The function windowing that applies the hamming window is implemented and used on the pre emphasized frames.</p><pre class="codeinput">[windowed_frames, window] = windowing(preemph);
test_windowed = example{1,1}.windowed';
</pre><p>The shape of the Hamming window used is figure;</p><pre class="codeinput">wvtool(window)
</pre><img vspace="5" hspace="5" src="Feature_Extraction_03.png" alt=""> <p>We now have to check the performance of the written function. To do so, we have to compare the output of this function with the test data.</p><pre class="codeinput">figure;
subplot(2,1,1)
imagesc(windowed_frames)
colormap <span class="string">jet</span>
title(<span class="string">' Windowing function output'</span>)
subplot(2,1,2)
imagesc(test_windowed)
colormap <span class="string">jet</span>
title(<span class="string">' Test windowing set'</span>)
</pre><img vspace="5" hspace="5" src="Feature_Extraction_04.png" alt=""> <p>To check the performance numerically, we will substract the test data from the output and get the total differences between both of them</p><pre class="codeinput">subs = test_windowed - windowed_frames;
window_error = sum(sum(subs))
</pre><pre class="codeoutput">
window_error =

    -7.791701311932187e-13

</pre><p>We can observe that the error is not exactly cero but is sufficiently small enough. The insignificant difference is probably due to the use of Matlab instead of Python.</p><h2 id="15">1.4 Fast Fourier Transform</h2><p>The powerSpectrum function is developed in this section. This function performs the Fast fourier Transform to the windowed frames and a then applies a squared power to the modulus. The FFT samples length is 512.</p><p>As now er are in the frequency domain, it is interesting what the max frequency will be:</p><p><img src="Feature_Extraction_eq17991088040002015184.png" alt="$$f_{max}=\frac{f_{samp}}{2}=\frac{20000}{2} = 10 kHz$$"></p><pre class="codeinput">nfft = 512;
FFT_frame = powerSpectrum(windowed_frames,nfft);
test_FFT = example{1,1}.spec';
</pre><p>We have now to check the performance of the written function. To do so, we have to compare the output of this function with the test data.</p><pre class="codeinput">figure;
subplot(2,1,1)
imagesc(FFT_frame)
title(<span class="string">' Power Spectrum output'</span>)
subplot(2,1,2)
imagesc(test_FFT)
title(<span class="string">' Test Power Spectrum set'</span>)
</pre><img vspace="5" hspace="5" src="Feature_Extraction_05.png" alt=""> <p>To check the performance numerically, we will substract the test data from the output and get the total differences between both of them</p><pre class="codeinput">subs = test_FFT - FFT_frame;
FFT_error = sum(sum(subs))
</pre><pre class="codeoutput">
FFT_error =

    -2.497053070175601e-07

</pre><p>As expected, the error is not 0 as we get the errors from the previous step, but squared. Still the error is too small to be considered.</p><h2 id="19">1.5 Mel filterbank log spectrum</h2><p>A filter bank is a set a filters. In the case of the Mel fiterbank, these filters are triangles, in which the first filter only keeps low frequencies while the next filters slowly shift to higher frequencies and the amplitude of the triangles decrease, and they get wider.</p><p>This decreased amplitude try to represent the human ear system, as the higher the frequencies, the more difficult discern differences. This is why the triangles get wider: we need a bigger filter as the resolution is less precise.</p><p>The sum of the energy in every filter is computed. We need to take the log of this energy due to the log property of sound: to double the sound we need 8 times more energy.</p><p>In this case the number of filters in the bank is 40: 13 linear filters and 27 logaritmic.</p><pre class="codeinput">sampling_freq = 20000;
[melSpec_frames, filterBank] = logMelSpectrum(FFT_frame,sampling_freq);
test_melSpec = example{1,1}.mspec';
</pre><p>The Mel filter bank computed is the one that follows</p><pre class="codeinput">figure(<span class="string">'Name'</span>,<span class="string">'Mel Frequency Filter Bank'</span>)
plot(filterBank')
axis([0, 178,  0, max(max(filterBank))])
title(<span class="string">'Mel-filterbank'</span>)
</pre><img vspace="5" hspace="5" src="Feature_Extraction_06.png" alt=""> <p>We have now to check the performance of the written function. To do so, we have to compare the output of this function with the test data.</p><pre class="codeinput">figure;
subplot(2,1,1)
imagesc(melSpec_frames)
title(<span class="string">' Power Spectrum output'</span>)
colormap <span class="string">jet</span>
subplot(2,1,2)
imagesc(test_melSpec)
title(<span class="string">' Test Power Spectrum set'</span>)
colormap <span class="string">jet</span>
</pre><img vspace="5" hspace="5" src="Feature_Extraction_07.png" alt=""> <p>To check the performance numerically, we will substract the test data from the output and get the total differences between both of them</p><pre class="codeinput">subs = test_melSpec - melSpec_frames;
melSpec_error = sum(sum(subs))
</pre><pre class="codeoutput">
melSpec_error =

     3.582160609805207e-14

</pre><h2 id="23">1.6 Cosine Transform</h2><p>The Discrete Cosine Transform perform a similar operation than the Fourier transform: they both descompose a discrete-time vector in a sum of scaled-and-shifted basis functions. The main difference is that the DCT uses only cosines as the Basis function. We get then, for each window, 13 coefficients or cepstrums.</p><pre class="codeinput">number_of_coefficients = 13;
[CT_frames] = cepstrum(melSpec_frames, number_of_coefficients);
test_CT = example{1,1}.mfcc';
</pre><p>We have now to check the performance of the written function. To do so, we have to compare the output of this function with the test data.</p><pre class="codeinput">figure;
subplot(2,1,1)
imagesc(CT_frames)
title(<span class="string">' Cosine transformation output'</span>)
colormap <span class="string">jet</span>
subplot(2,1,2)
imagesc(test_CT)
title(<span class="string">' Test Cosine transformation set'</span>)
colormap <span class="string">jet</span>
</pre><img vspace="5" hspace="5" src="Feature_Extraction_08.png" alt=""> <p>To check the performance numerically, we will substract the test data from the output and get the total differences between both of them</p><pre class="codeinput">subs = test_CT - CT_frames;
CT_error = sum(sum(subs))
</pre><pre class="codeoutput">
CT_error =

     4.555982327514307e-13

</pre><h2 id="26">1.7 Liftering</h2><p>Fhis is the last step, which output are the final MFCCs for the current utterance. A lifter is a filter that operates on a cepstrum might be called a lifter. A low-pass lifter is similar to a low-pass filter in the frequency domain. This is done to even clean the signal more leaving only the important and different information.</p><pre class="codeinput">[Lift_frames] = lifter_matlab(CT_frames);
test_Lifter = example{1,1}.lmfcc';
</pre><p>We have now to check the performance of the written function. To do so, we have to compare the output of this function with the test data.</p><pre class="codeinput">figure;
subplot(2,1,1)
imagesc(Lift_frames)
title(<span class="string">' Lifter output'</span>)
colormap <span class="string">jet</span>
subplot(2,1,2)
imagesc(test_Lifter)
title(<span class="string">' Test Lifter set'</span>)
colormap <span class="string">jet</span>
</pre><img vspace="5" hspace="5" src="Feature_Extraction_09.png" alt=""> <p>To check the performance numerically, we will substract the test data from the output and get the total differences between both of them</p><pre class="codeinput">subs = test_Lifter - Lift_frames;
Lift_error = sum(sum(subs))
</pre><pre class="codeoutput">
Lift_error =

     8.972197984569164e-13

</pre><p>All the steps together in order can be represented by the following figure:</p><pre class="codeinput">figure
plot(samples)
figure
imagesc(frames);
colormap(jet)
figure
imagesc(preemph)
colormap(jet)
figure
imagesc(windowed_frames)
colormap(jet)
figure
imagesc(FFT_frame)
colormap <span class="string">default</span>
figure
imagesc(melSpec_frames)
colormap(jet)
figure
imagesc(CT_frames)
colormap(jet)
figure
imagesc(Lift_frames)
colormap(jet)
</pre><img vspace="5" hspace="5" src="Feature_Extraction_10.png" alt=""> <img vspace="5" hspace="5" src="Feature_Extraction_11.png" alt=""> <img vspace="5" hspace="5" src="Feature_Extraction_12.png" alt=""> <img vspace="5" hspace="5" src="Feature_Extraction_13.png" alt=""> <img vspace="5" hspace="5" src="Feature_Extraction_14.png" alt=""> <img vspace="5" hspace="5" src="Feature_Extraction_15.png" alt=""> <img vspace="5" hspace="5" src="Feature_Extraction_16.png" alt=""> <img vspace="5" hspace="5" src="Feature_Extraction_17.png" alt=""> <h2 id="30">1.8 Calculation of the MFCCs for each uterance</h2><p>For the further analisys of the data set, all the utterance are going to be transformed to the MFCCs space. Also, the output of each uterance will be concatenated in an unique matrix of features in order to be able to represent and gather all the features included in the data set. This matrix has dimensions of NxM being N the total number of frames in the data set and M the number of coefficients (13).</p><pre class="codeinput">gender = [];
speaker = [];
digits = [];
MFCCs = cell(1,44);
mSpec = cell(1,44);
MFCCs_straight = cell(1,44);
mSpec_straight = cell(1,44);
MFCCs_concatenate = [];
mSpec_concatenate = [];
<span class="keyword">for</span> i = 1:size(tidigits,2)
    [MFCCs_straight{i} mSpec_straight{i}] = mfcc(tidigits{1,i}.samples);
    MFCCs{i} = MFCCs_straight{i}';
    MFCCs_concatenate = vertcat(MFCCs_concatenate,MFCCs{i});
    mSpec{i} = mSpec_straight{i}';
    mSpec_concatenate = vertcat(mSpec_concatenate,mSpec{i});
    digits =[digits; tidigits{1,i}.digit,tidigits{1,i}.gender(1)];
    gender = [gender tidigits{1,i}.gender(1)];
    speaker = [speaker; tidigits{1,i}.speaker];
<span class="keyword">end</span>
</pre><h2 id="31">2. Study the correlation between uterances</h2><p>To study the correlation between the 13 coefficients, the correlation matrix of the entire feature matrix is calculated using the MFCCs coefficients.</p><pre class="codeinput">figure
corMFCCs = corrcoef(MFCCs_concatenate);
imagesc(flipdim(corMFCCs ,1));           <span class="comment">%# vertical flip)</span>
axis <span class="string">equal</span>
colormap <span class="string">winter</span>
colorbar
</pre><img vspace="5" hspace="5" src="Feature_Extraction_18.png" alt=""> <p>And the Mel filterbank features</p><pre class="codeinput">figure
corMspec = corrcoef(mSpec_concatenate);
imagesc(flipdim(corMspec ,1));           <span class="comment">%# vertical flip)</span>
axis <span class="string">equal</span>
colormap <span class="string">winter</span>
colorbar
</pre><img vspace="5" hspace="5" src="Feature_Extraction_19.png" alt=""> <p>By comparing both results we can justify the steps that follow the application of the filterbank as all the correlations between coefficients have almost disappeared meaning this that all the coeffiecients are giving unique information. We can accept the diagonal covariance matrices assumption for Gaussian models as the MFCCs coefficients are almost not correlated at all.</p><h2 id="34">3. Distances</h2><p>The first step is that, given 2 utterances's MFCCs matrices of NxM each of the same word pronnounce by the same speaker but in differente repetitions, compute de Euclidian distance between them.</p><pre class="codeinput">Rep_oa = mfcc(tidigits{1}.samples)';
Rep_ob = mfcc(tidigits{2}.samples)';

local_dist_o = localDistances(Rep_oa, Rep_ob);
figure
imagesc(local_dist_o)
xlabel(<span class="string">' Repetition B - word o'</span> )
ylabel(<span class="string">' Repetition A - word o'</span> )
colormap <span class="string">jet</span>
colorbar
</pre><img vspace="5" hspace="5" src="Feature_Extraction_20.png" alt=""> <p>We can observe that in the first repetition the speaker said the word faster as the square that represent when the word is pronnounced and therefore where the max distances are, is moved towards the upleft corner.</p><p>The last step is to apply the Dynamic Time Warping to compute the global distances between all the utterances's MFCCs. This steo is done in order to study the relation between the digits, speakers and even genres and doing a later clustering.</p><pre class="codeinput">D = zeros(size(tidigits,2),size(tidigits,2));
<span class="keyword">for</span> i = 1:size(tidigits,2)
    <span class="keyword">for</span> j =  1:size(tidigits,2)
        a = MFCCs{i};
        b = MFCCs{j};
        local_dist = localDistances(a, b);
        D(i,j) = dtw(local_dist);
    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><p>Once this Global distances matrix is computed, is time to plot it:</p><pre class="codeinput">figure;
imagesc(D)
<span class="comment">% colormap default(1000))</span>
colormap(jet(60))
colorbar;
labels = tidigit2labels(tidigits);
set(gca,<span class="string">'XTick'</span>,1:44);
set(gca,<span class="string">'XTickLabel'</span>,labels);
set(gca,<span class="string">'XTickLabelRotation'</span>,90);
set(gca,<span class="string">'YTick'</span>,1:44);
set(gca,<span class="string">'YTickLabel'</span>,labels);
set(gca,<span class="string">'YTickLabelRotation'</span>,0);
</pre><pre class="codeoutput">
labels =

     []


nex =

    44

</pre><img vspace="5" hspace="5" src="Feature_Extraction_21.png" alt=""> <p>First, is really easy to see something that was evident: the diagonal is zero as it represents the distance between the same utterances. But also, we can observe that the distances between repetitions of the same speaker are really low. We can also observe that the upperleft square corresponds to the lower distances meaning that in general, between the mal speakers there are more similarities than between female spakers</p><h2 id="39">Linkage clustering</h2><p>Now the objective is to achieve a hierarchical clustering over the Global distance matrix. To do this we use the Linkage Matlab function over the distance matrix and compute the dendrogram with the outputed data.</p><pre class="codeinput">Clusters = linkage(D,<span class="string">'complete'</span>);
[H,T,outperm] = dendrogram(Clusters, 0);
labels = labels(outperm,:);
set(gca,<span class="string">'XTick'</span>,1:44);
set(gca,<span class="string">'XTickLabel'</span>,labels);
set(gca,<span class="string">'XTickLabelRotation'</span>,90);
</pre><img vspace="5" hspace="5" src="Feature_Extraction_22.png" alt=""> <p>We can observe that the the classification of the digits spoken by the same speaker is generally well done, as in the first clustering level, same digits frome the same speaker are in the same cluster, with some exceptions: numbers 9 and 8. Also, in the second clusterin level we can see that some digits appear together for both speakers as for the digits 2 and 3. The last apreciation will be the fact that man and women are separated in big clusters.</p><h2 id="41">Train</h2><pre class="codeinput">groups = cell(4);
character = cell(4);
count = 1;
groups_concatenate = [];
<span class="keyword">for</span> i = 1:size(tidigits,2)
    <span class="keyword">if</span> tidigits{i}.digit == <span class="string">'7'</span>
        groups{count} = mfcc(tidigits{i}.samples)';
        groups_concatenate = vertcat(groups_concatenate,groups{count});
        character{count} = tidigits{i}.digit;
        count = count + 1;
    <span class="keyword">end</span>
<span class="keyword">end</span>

D = zeros(size(groups,2),size(groups,2));
<span class="keyword">for</span> i = 1:size(groups,2)
    <span class="keyword">for</span> j =  1:size(groups,2)
        a = groups{i};
        b = groups{j};
        local_dist = localDistances(a, b);
        D(i,j) = dtw(local_dist);
    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><pre class="codeinput">componentsNr = [4 8 16 32];
<span class="comment">% data = MFCCs{1};</span>
<span class="comment">% data = tidigits{1}.samples';</span>
data_t = groups_concatenate;
data = MFCCs_concatenate;

size(data)

options = statset(<span class="string">'Display'</span>,<span class="string">'final'</span>);
GMModel = fitgmdist(data_t,componentsNr(1),<span class="string">'Options'</span>,options)

<span class="comment">% h = ezcontour(@(x,y)pdf(obj,[x y]),[-8 6],[-8 6]);</span>
P = posterior(GMModel,groups{1});
size(P)

figure
subplot(2,1,1)
imagesc(groups{1}')
subplot(2,1,2)
imagesc(P')
</pre><pre class="codeoutput">
ans =

        3883          13

20 iterations, log-likelihood = -18809.6

GMModel = 

Gaussian mixture distribution with 4 components in 13 dimensions



ans =

    92     4

</pre><img vspace="5" hspace="5" src="Feature_Extraction_23.png" alt=""> <pre class="codeinput">componentsNr = [4 8 16 32];
<span class="comment">% data = MFCCs{4};</span>
<span class="comment">% data = tidigits{1}.samples';</span>
<span class="comment">% data = groups_concatenate;</span>
data = MFCCs_concatenate;
data_t = groups_concatenate;
size(data)

options = statset(<span class="string">'Display'</span>,<span class="string">'final'</span>);
GMModel = fitgmdist(data,componentsNr(3),<span class="string">'Options'</span>,options)

<span class="comment">% h = ezcontour(@(x,y)pdf(obj,[x y]),[-8 6],[-8 6]);</span>
groups_7 = cell(4);
character2 = cell(4);
count = 1;
groups_concatenate_7 = [];
<span class="keyword">for</span> i = 1:size(tidigits,2)
    <span class="keyword">if</span> tidigits{i}.digit == <span class="string">'7'</span>
        groups_7{count} = mfcc(tidigits{i}.samples)';
        groups_concatenate_7 = vertcat(groups_concatenate,groups_7{count});
        character{count} = tidigits{i}.digit;
        count = count + 1;
    <span class="keyword">end</span>
<span class="keyword">end</span>
<span class="comment">% groups_concatenate_7</span>
size(groups_concatenate_7)
mfcc_rows = size(groups_concatenate_7, 1) /4;
mfcc_cols = size(groups_concatenate_7, 2) / 1;
test1 = groups_concatenate_7(1:mfcc_rows, 1:mfcc_cols);
<span class="comment">% P = posterior(GMModel,data_t);</span>
P = posterior(GMModel,test1);
<span class="comment">% P</span>
size(P)

figure
subplot(2,1,1)
imagesc(test1')
subplot(2,1,2)
imagesc(P')
</pre><pre class="codeoutput">
ans =

        3883          13

56 iterations, log-likelihood = -190764

GMModel = 

Gaussian mixture distribution with 16 components in 13 dimensions



ans =

   484    13


ans =

   121    16

</pre><img vspace="5" hspace="5" src="Feature_Extraction_24.png" alt=""> <p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2016b</a><br></p></div><!--
##### SOURCE BEGIN #####
%% DT2119 - Speech and speaker recognition - Lab 1 - Feature extraction

%%
% Mel Frequency Cepstral Coefficients (MFCCs) are coefficients used in
% Speech recognition based on human auditive perception. These coefficients
% come from the need, in the field of autonomous audio recognition, to
% extract the main features of an audio signal while discarding all the
% irrelevant features that will make the recognition harder to achieve
% (background noise, emotion ...)
%
% The sounds generated by a human depend on the shape of the vocal tract,
% position of the tongue, teeth, lips etc. Then, if we can determine this
% shape accurately, the phoneme that was produced will be easy to identify.
% The main objetive of the MFCC it to accurately represent the envelope of
% the short time power spectrum in order to stablish the shape of the vocal
% tract. Consisting then of one of the most important features in Speech
% recognition

%%
% First we need to prepare the workspace and load the data set
clear all;
close all; 
load('tidigits.mat')
load('example.mat')


%% 1.1 Enframe the audio signal
%
% The purpose of this first step is to cut the original sample in many
% smaller ones. In this case, we want windowframes of 20 ms with a shift
% between windows of 10 ms. As the shift is smaller than the window, the
% computed frames will have shared parts between each other
%
% *Compute the number of samples per frame and shift*
% 
% As out signal has been sampled, we dont have a continuos signal but a
% discrete one. This is why we are interested of knowing the number of
% samples per window rather than the time. 
%
% The sampling rate is S = 20 kHz. Then the period is $T = 1/S$. 
% We have that: 
%
% Length of the window: $0.02T = 400$ samples
% Length of the shift: $0.01T = 200$ samples


winlen = 400;
winshift = 200; 

%%
% Now is time to check the performance of the written function. To do so,
% the function is evaluated with the example's structure data set, and compared to the
% frames given in the example's structure frames. The output generated must
% be identical to the frames stored in the data structure in example.

samples = example{1,1}.samples; 
test = example{1,1}.frames';
frames = enframe(samples,winlen,winshift);
% As the image have to be plotted in with the same orientation as in the
% test data then
frames = frames(:,1:400)'; 
%% 
% The comparison is shown in the folowwing figure. 
subplot(2,1,1)
imagesc(frames)
colormap jet
title(' Enframe function output')
subplot(2,1,2)
imagesc(test)
colormap jet
title(' Test frames set')

%%
% To check the performance numerically, we will substract the test data
% from the output and get the total differences between both of them

subs = test - frames;
Enframe_error = sum(sum(subs))
%% 1.2 Pre-emphasys
%
% The main objetive of this function is to compesate the 6dB/octave that
% are dropped due to the radiation at the lips. This funcion is: 
%
% $y[n] = x[n] - ax[n-1]$
%
% Being the coefficientes  $A = 1$ and $B = [1 -a]$.
%
% The purpose of this filter is to filter the lower frequencies, letting pass the higher ones. 
% This is because the important features are in the higher frequencies, remaining
% the lower frequencies almost unchanged. Therefore, by doing the filtering
% we discard the similar parts of the signal analyzing only the higher
% frequencies, where the important features are. 
%
% We have now to check the performance of the written function. To do so,
% we have to compare the output of this function with the test data. 

a = 0.97;
preemph = preemp(frames,a);
test_preem = example{1,1}.preemph';

%% 
% The comparison is shown in the following figure. 
figure;
subplot(2,1,1)
imagesc(preemph)
colormap jet
title(' Preemp function output')
subplot(2,1,2)
imagesc(test_preem)
colormap jet
title(' Test preemphashis set')

%%
% To check the performance numerically, we will substract the test data
% from the output and get the total differences between both of them

subs = test_preem - preemph;
Preemphasis_error = sum(sum(subs))

%% 1.3 Hamming window
%
% The use of the Hamming window is justified as it emphasizes the center of
% the window, improving this way the values of the Fourier Transfer
% Function. The sidelobes are reduced, being the center lobe much more important
% than these side lobes. 
%
% The Hamming window is used in order to reduce the discontinuities in the
% edges of the frames, focusing in the main(centered) frequencies. 
%
% The function windowing that applies the hamming window is implemented and
% used on the pre emphasized frames. 

[windowed_frames, window] = windowing(preemph);
test_windowed = example{1,1}.windowed';
%%
% The shape of the Hamming window used is
% figure;
wvtool(window)

%%
% We now have to check the performance of the written function. To do so,
% we have to compare the output of this function with the test data.
figure;
subplot(2,1,1)
imagesc(windowed_frames)
colormap jet
title(' Windowing function output')
subplot(2,1,2)
imagesc(test_windowed)
colormap jet
title(' Test windowing set')

%%
% To check the performance numerically, we will substract the test data
% from the output and get the total differences between both of them

subs = test_windowed - windowed_frames;
window_error = sum(sum(subs))

%%
% We can observe that the error is not exactly cero but is sufficiently
% small enough. The insignificant difference is probably due to the use of
% Matlab instead of Python. 

%% 1.4 Fast Fourier Transform
%
% The powerSpectrum function is developed in this section. This function
% performs the Fast fourier Transform to the windowed frames and a then
% applies a squared power to the modulus. The FFT samples length is 512.
%
% As now er are in the frequency domain, it is interesting what the max
% frequency will be: 
%
% $$f_{max}=\frac{f_{samp}}{2}=\frac{20000}{2} = 10 kHz$$
nfft = 512;
FFT_frame = powerSpectrum(windowed_frames,nfft);
test_FFT = example{1,1}.spec';

%%
% We have now to check the performance of the written function. To do so,
% we have to compare the output of this function with the test data.
figure;
subplot(2,1,1)
imagesc(FFT_frame)
title(' Power Spectrum output')
subplot(2,1,2)
imagesc(test_FFT)
title(' Test Power Spectrum set')

%%
% To check the performance numerically, we will substract the test data
% from the output and get the total differences between both of them

subs = test_FFT - FFT_frame;
FFT_error = sum(sum(subs))

%%
% As expected, the error is not 0 as we get the errors from the previous
% step, but squared. Still the error is too small to be considered.

%% 1.5 Mel filterbank log spectrum
%
% A filter bank is a set a filters. In the case of the Mel fiterbank, these
% filters are triangles, in which the first filter only keeps low
% frequencies while the next filters slowly shift to higher frequencies and
% the amplitude of the triangles decrease, and they get wider. 
%
% This decreased amplitude try to represent the human ear system, as the
% higher the frequencies, the more difficult discern differences. This is
% why the triangles get wider: we need a bigger filter as the resolution is
% less precise.
%
% The sum of the energy in every filter is computed. We need to take the
% log of this energy due to the log property of sound: to double the sound
% we need 8 times more energy. 
% 
% In this case the number of filters in the bank is 40: 13 linear filters
% and 27 logaritmic.

sampling_freq = 20000;
[melSpec_frames, filterBank] = logMelSpectrum(FFT_frame,sampling_freq);
test_melSpec = example{1,1}.mspec';

%%
% The Mel filter bank computed is the one that follows
figure('Name','Mel Frequency Filter Bank')
plot(filterBank')
axis([0, 178,  0, max(max(filterBank))])
title('Mel-filterbank')
%%
% We have now to check the performance of the written function. To do so,
% we have to compare the output of this function with the test data.
figure;
subplot(2,1,1)
imagesc(melSpec_frames)
title(' Power Spectrum output')
colormap jet
subplot(2,1,2)
imagesc(test_melSpec)
title(' Test Power Spectrum set')
colormap jet

%%
% To check the performance numerically, we will substract the test data
% from the output and get the total differences between both of them
subs = test_melSpec - melSpec_frames;
melSpec_error = sum(sum(subs))

%% 1.6 Cosine Transform
%
% The Discrete Cosine Transform perform a similar operation than the
% Fourier transform: they both descompose a discrete-time vector in a sum
% of scaled-and-shifted basis functions. The main difference is that the
% DCT uses only cosines as the Basis function. We get then, for each
% window, 13 coefficients or cepstrums. 

number_of_coefficients = 13;
[CT_frames] = cepstrum(melSpec_frames, number_of_coefficients);
test_CT = example{1,1}.mfcc';

%%
% We have now to check the performance of the written function. To do so,
% we have to compare the output of this function with the test data.
figure;
subplot(2,1,1)
imagesc(CT_frames)
title(' Cosine transformation output')
colormap jet
subplot(2,1,2)
imagesc(test_CT)
title(' Test Cosine transformation set')
colormap jet

%%
% To check the performance numerically, we will substract the test data
% from the output and get the total differences between both of them
subs = test_CT - CT_frames;
CT_error = sum(sum(subs))

%% 1.7 Liftering
%
% Fhis is the last step, which output are the final MFCCs for the current utterance.
% A lifter is a filter that operates on a cepstrum might be called a lifter. A low-pass lifter 
% is similar to a low-pass filter in the frequency domain. This is done to
% even clean the signal more leaving only the important and different
% information. 

[Lift_frames] = lifter_matlab(CT_frames);
test_Lifter = example{1,1}.lmfcc';
%%
% We have now to check the performance of the written function. To do so,
% we have to compare the output of this function with the test data.
figure;
subplot(2,1,1)
imagesc(Lift_frames)
title(' Lifter output')
colormap jet
subplot(2,1,2)
imagesc(test_Lifter)
title(' Test Lifter set')
colormap jet

%%
% To check the performance numerically, we will substract the test data
% from the output and get the total differences between both of them
subs = test_Lifter - Lift_frames;
Lift_error = sum(sum(subs))

%%
%
% All the steps together in order can be represented by the following
% figure: 

figure
plot(samples)
figure
imagesc(frames);
colormap(jet)
figure
imagesc(preemph)
colormap(jet)
figure
imagesc(windowed_frames)
colormap(jet)
figure
imagesc(FFT_frame)
colormap default
figure
imagesc(melSpec_frames)
colormap(jet)
figure
imagesc(CT_frames)
colormap(jet)
figure
imagesc(Lift_frames)
colormap(jet)
%% 1.8 Calculation of the MFCCs for each uterance
%
% For the further analisys of the data set, all the utterance are going to
% be transformed to the MFCCs space. Also, the output of each uterance will
% be concatenated in an unique matrix of features in order to be able to
% represent and gather all the features included in the data set. This
% matrix has dimensions of NxM being N the total number of frames in the
% data set and M the number of coefficients (13).

gender = [];
speaker = [];
digits = [];
MFCCs = cell(1,44);
mSpec = cell(1,44);
MFCCs_straight = cell(1,44);
mSpec_straight = cell(1,44);
MFCCs_concatenate = [];
mSpec_concatenate = [];
for i = 1:size(tidigits,2)
    [MFCCs_straight{i} mSpec_straight{i}] = mfcc(tidigits{1,i}.samples);
    MFCCs{i} = MFCCs_straight{i}';
    MFCCs_concatenate = vertcat(MFCCs_concatenate,MFCCs{i});
    mSpec{i} = mSpec_straight{i}';
    mSpec_concatenate = vertcat(mSpec_concatenate,mSpec{i});
    digits =[digits; tidigits{1,i}.digit,tidigits{1,i}.gender(1)];
    gender = [gender tidigits{1,i}.gender(1)];
    speaker = [speaker; tidigits{1,i}.speaker];
end

%% 2. Study the correlation between uterances
%
% To study the correlation between the 13 coefficients, the correlation
% matrix of the entire feature matrix is calculated using the MFCCs
% coefficients. 
figure
corMFCCs = corrcoef(MFCCs_concatenate);
imagesc(flipdim(corMFCCs ,1));           %# vertical flip)
axis equal
colormap winter
colorbar

%% 
%
% And the Mel filterbank features

figure
corMspec = corrcoef(mSpec_concatenate);
imagesc(flipdim(corMspec ,1));           %# vertical flip)
axis equal
colormap winter
colorbar


%%
%
% By comparing both results we can justify the steps that follow the
% application of the filterbank as all the correlations between
% coefficients have almost disappeared meaning this that all the
% coeffiecients are giving unique information. We can accept the diagonal
% covariance matrices assumption for Gaussian models as the MFCCs
% coefficients are almost not correlated at all. 
%% 3. Distances
%
% The first step is that, given 2 utterances's MFCCs matrices of NxM each
% of the same word pronnounce by the same speaker but in differente repetitions, compute de
% Euclidian distance between them. 

Rep_oa = mfcc(tidigits{1}.samples)';
Rep_ob = mfcc(tidigits{2}.samples)';

local_dist_o = localDistances(Rep_oa, Rep_ob);
figure
imagesc(local_dist_o)
xlabel(' Repetition B - word o' )
ylabel(' Repetition A - word o' )
colormap jet
colorbar

%%
%
% We can observe that in the first repetition the speaker said the word faster as the square that
% represent when the word is pronnounced and therefore where the max
% distances are, is moved towards the upleft corner. 
%%
% 
% The last step is to apply the Dynamic Time Warping to compute the global
% distances between all the utterances's MFCCs. This steo is done in order
% to study the relation between the digits, speakers and even genres and
% doing a later clustering. 

D = zeros(size(tidigits,2),size(tidigits,2));
for i = 1:size(tidigits,2)
    for j =  1:size(tidigits,2)
        a = MFCCs{i};
        b = MFCCs{j};
        local_dist = localDistances(a, b);
        D(i,j) = dtw(local_dist);
    end
end

%%
%
% Once this Global distances matrix is computed, is time to plot it:
figure;
imagesc(D)
% colormap default(1000))
colormap(jet(60))
colorbar;
labels = tidigit2labels(tidigits);
set(gca,'XTick',1:44);
set(gca,'XTickLabel',labels);
set(gca,'XTickLabelRotation',90);
set(gca,'YTick',1:44);
set(gca,'YTickLabel',labels);
set(gca,'YTickLabelRotation',0);

%%
% 
% First, is really easy to see something that was evident: the diagonal is
% zero as it represents the distance between the same utterances. But also,
% we can observe that the distances between repetitions of the same speaker
% are really low. We can also observe that the upperleft square corresponds
% to the lower distances meaning that in general, between the mal speakers
% there are more similarities than between female spakers

%% Linkage clustering
%
% Now the objective is to achieve a hierarchical clustering over the
% Global distance matrix. To do this we use the Linkage Matlab function over
% the distance matrix and compute the dendrogram with the outputed data.
Clusters = linkage(D,'complete');
[H,T,outperm] = dendrogram(Clusters, 0);
labels = labels(outperm,:);
set(gca,'XTick',1:44);
set(gca,'XTickLabel',labels);
set(gca,'XTickLabelRotation',90);

%%
%
% We can observe that the the classification of the digits spoken by the
% same speaker is generally well done, as in the first clustering level,
% same digits frome the same speaker are in the same cluster, with some
% exceptions: numbers 9 and 8. Also, in the second clusterin level we can
% see that some digits appear together for both speakers as for the digits
% 2 and 3. The last apreciation will be the fact that man and women are
% separated in big clusters. 
%% Train

groups = cell(4);
character = cell(4);
count = 1; 
groups_concatenate = [];
for i = 1:size(tidigits,2)
    if tidigits{i}.digit == '7'
        groups{count} = mfcc(tidigits{i}.samples)';
        groups_concatenate = vertcat(groups_concatenate,groups{count});
        character{count} = tidigits{i}.digit;
        count = count + 1;
    end
end

D = zeros(size(groups,2),size(groups,2));
for i = 1:size(groups,2)
    for j =  1:size(groups,2)
        a = groups{i};
        b = groups{j};
        local_dist = localDistances(a, b);
        D(i,j) = dtw(local_dist);
    end
end

%%
%

componentsNr = [4 8 16 32];
% data = MFCCs{1};
% data = tidigits{1}.samples';
data_t = groups_concatenate;
data = MFCCs_concatenate;

size(data)

options = statset('Display','final');
GMModel = fitgmdist(data_t,componentsNr(1),'Options',options)

% h = ezcontour(@(x,y)pdf(obj,[x y]),[-8 6],[-8 6]);
P = posterior(GMModel,groups{1});
size(P)

figure
subplot(2,1,1)
imagesc(groups{1}')
subplot(2,1,2)
imagesc(P')

%%

componentsNr = [4 8 16 32];
% data = MFCCs{4};
% data = tidigits{1}.samples';
% data = groups_concatenate;
data = MFCCs_concatenate;
data_t = groups_concatenate;
size(data)

options = statset('Display','final');
GMModel = fitgmdist(data,componentsNr(3),'Options',options)

% h = ezcontour(@(x,y)pdf(obj,[x y]),[-8 6],[-8 6]);
groups_7 = cell(4);
character2 = cell(4);
count = 1; 
groups_concatenate_7 = [];
for i = 1:size(tidigits,2)
    if tidigits{i}.digit == '7'
        groups_7{count} = mfcc(tidigits{i}.samples)';
        groups_concatenate_7 = vertcat(groups_concatenate,groups_7{count});
        character{count} = tidigits{i}.digit;
        count = count + 1;
    end
end
% groups_concatenate_7
size(groups_concatenate_7)
mfcc_rows = size(groups_concatenate_7, 1) /4;
mfcc_cols = size(groups_concatenate_7, 2) / 1;
test1 = groups_concatenate_7(1:mfcc_rows, 1:mfcc_cols);
% P = posterior(GMModel,data_t);
P = posterior(GMModel,test1);
% P
size(P)

figure
subplot(2,1,1)
imagesc(test1')
subplot(2,1,2)
imagesc(P')

##### SOURCE END #####
--></body></html>